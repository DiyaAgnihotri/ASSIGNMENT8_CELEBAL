# -*- coding: utf-8 -*-
"""ASSIGNMENT8_CELEBAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/196N3xgargUGdj6M-FQznkJtyxm6Yml6B
"""

import pandas as pd

df = pd.read_csv('Training Dataset.csv')

print("Shape of dataset:", df.shape)
print("\nFirst 5 rows:\n", df.head())

print("\nInfo:\n")
df.info()

print("\nMissing values:\n")
print(df.isnull().sum())

categoricals = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area', 'Loan_Status']
for col in categoricals:
    print(f"\n{col} distribution:\n", df[col].value_counts(dropna=False))

import os
import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pickle
import gzip
import json
from functools import lru_cache
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OptimizedLoanPipeline:
    def __init__(self):
        self.model = None
        self.label_encoders = {}
        self.embedder = None
        self.rag_embeddings = None
        self.rag_texts = []
        self.df_raw = None
        self.train_stats = {}

    def load_and_preprocess_train(self):
        """Optimized training data preprocessing with caching"""
        logger.info("Loading training data...")
        df = pd.read_csv('Training Dataset.csv')

        df_rag = df.copy()

        fill_values = {
            'Gender': df['Gender'].mode()[0] if not df['Gender'].mode().empty else 'Male',
            'Married': df['Married'].mode()[0] if not df['Married'].mode().empty else 'No',
            'Dependents': df['Dependents'].mode()[0] if not df['Dependents'].mode().empty else '0',
            'Self_Employed': df['Self_Employed'].mode()[0] if not df['Self_Employed'].mode().empty else 'No',
            'LoanAmount': df['LoanAmount'].median(),
            'Loan_Amount_Term': df['Loan_Amount_Term'].mode()[0] if not df['Loan_Amount_Term'].mode().empty else 360,
            'Credit_History': df['Credit_History'].mode()[0] if not df['Credit_History'].mode().empty else 1
        }
        df.fillna(fill_values, inplace=True)

        df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})
        df['Dependents'] = df['Dependents'].replace('3+', '3').astype(int)

        categorical_cols = ['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area']
        for col in categorical_cols:
            if col not in self.label_encoders:
                self.label_encoders[col] = LabelEncoder()
                df[col] = self.label_encoders[col].fit_transform(df[col])
            else:
                df[col] = self.label_encoders[col].transform(df[col])

        self._precompute_stats(df_rag)

        X = df.drop(columns=['Loan_ID', 'Loan_Status'])
        y = df['Loan_Status']

        logger.info(f"Training data loaded: {X.shape[0]} samples, {X.shape[1]} features")
        return X, y, df_rag

    def _precompute_stats(self, df_raw):
        """Precompute approval statistics for faster RAG responses"""
        logger.info("Precomputing approval statistics...")

        df_stats = df_raw.fillna({
            'Gender': df_raw['Gender'].mode()[0] if not df_raw['Gender'].mode().empty else 'Male',
            'Married': df_raw['Married'].mode()[0] if not df_raw['Married'].mode().empty else 'No',
            'Education': df_raw['Education'].mode()[0] if not df_raw['Education'].mode().empty else 'Graduate',
            'Self_Employed': df_raw['Self_Employed'].mode()[0] if not df_raw['Self_Employed'].mode().empty else 'No',
            'Property_Area': df_raw['Property_Area'].mode()[0] if not df_raw['Property_Area'].mode().empty else 'Urban'
        })

        df_stats['Loan_Status'] = df_stats['Loan_Status'].map({'Y': 1, 'N': 0})

        columns_to_analyze = ['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area']

        for col in columns_to_analyze:
            if col in df_stats.columns:
                approval_stats = df_stats.groupby(col)['Loan_Status'].agg(['count', 'sum'])
                approval_stats['rate'] = (approval_stats['sum'] / approval_stats['count'] * 100).round(2)
                self.train_stats[col] = approval_stats['rate'].to_dict()

    def load_and_preprocess_test(self):
        """Optimized test data preprocessing"""
        logger.info("Loading test data...")
        df = pd.read_csv('Test Dataset.csv')

        fill_values = {
            'Gender': df['Gender'].mode()[0] if not df['Gender'].mode().empty else 'Male',
            'Married': df['Married'].mode()[0] if not df['Married'].mode().empty else 'No',
            'Dependents': df['Dependents'].mode()[0] if not df['Dependents'].mode().empty else '0',
            'Self_Employed': df['Self_Employed'].mode()[0] if not df['Self_Employed'].mode().empty else 'No',
            'LoanAmount': df['LoanAmount'].median(),
            'Loan_Amount_Term': df['Loan_Amount_Term'].mode()[0] if not df['Loan_Amount_Term'].mode().empty else 360,
            'Credit_History': df['Credit_History'].mode()[0] if not df['Credit_History'].mode().empty else 1
        }
        df.fillna(fill_values, inplace=True)

        df['Dependents'] = df['Dependents'].replace('3+', '3').astype(int)

        categorical_cols = ['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area']
        for col in categorical_cols:
            if col in self.label_encoders:
                df[col] = df[col].map(lambda x: x if x in self.label_encoders[col].classes_ else self.label_encoders[col].classes_[0])
                df[col] = self.label_encoders[col].transform(df[col])
            else:
                logger.warning(f"No encoder found for {col}, using default encoding")
                le = LabelEncoder()
                df[col] = le.fit_transform(df[col])

        loan_ids = df['Loan_ID']
        X_test = df.drop(columns=['Loan_ID'])

        logger.info(f"Test data loaded: {X_test.shape[0]} samples")
        return X_test, loan_ids

    def train_and_predict(self, X, y, X_test):
        """Optimized training and prediction"""
        logger.info("Training XGBoost model...")

        self.model = xgb.XGBClassifier(
            n_estimators=50,
            learning_rate=0.2,
            max_depth=3,
            random_state=42,
            use_label_encoder=False,
            eval_metric='logloss',
            n_jobs=-1,
            tree_method='hist'
        )

        self.model.fit(X, y)
        logger.info("Model training completed")

        logger.info("Making predictions...")
        preds = self.model.predict(X_test)
        preds = ['Y' if p == 1 else 'N' for p in preds]

        return preds

    def row_to_text_optimized(self, row):
        gender_map = {0: 'Male', 1: 'Female', 'Male': 'Male', 'Female': 'Female'}
        married_map = {0: 'not married', 1: 'married', 'No': 'not married', 'Yes': 'married'}
        education_map = {0: 'Graduate', 1: 'Not Graduate', 'Graduate': 'Graduate', 'Not Graduate': 'Not Graduate'}
        employed_map = {0: 'not self-employed', 1: 'self-employed', 'No': 'not self-employed', 'Yes': 'self-employed'}
        area_map = {0: 'Rural', 1: 'Semiurban', 2: 'Urban', 'Rural': 'Rural', 'Semiurban': 'Semiurban', 'Urban': 'Urban'}
        status_map = {0: 'rejected', 1: 'approved', 'N': 'rejected', 'Y': 'approved'}

        gender = gender_map.get(row.get('Gender', 'Male'), 'Male')
        married = married_map.get(row.get('Married', 'No'), 'not married')
        education = education_map.get(row.get('Education', 'Graduate'), 'Graduate')
        self_employed = employed_map.get(row.get('Self_Employed', 'No'), 'not self-employed')
        property_area = area_map.get(row.get('Property_Area', 'Urban'), 'Urban')
        loan_status = status_map.get(row.get('Loan_Status', 'N'), 'rejected')

        return (
            f"A {gender} applicant who is {married}, has {row.get('Dependents', 0)} dependents, "
            f"is a {education} and {self_employed}. Income: ₹{row.get('ApplicantIncome', 0)} with "
            f"co-applicant income: ₹{row.get('CoapplicantIncome', 0)}. Loan: ₹{row.get('LoanAmount', 0)} "
            f"for {row.get('Loan_Amount_Term', 360)} months, credit history: {row.get('Credit_History', 1)}, "
            f"property: {property_area}. Status: {loan_status}."
        )

    def save_compressed_rag_cache(self, cache_file='rag_cache_compressed.gz'):
        """Save RAG data with maximum compression"""
        cache_data = {
            'texts': self.rag_texts,
            'embeddings': self.rag_embeddings.astype(np.float16).tolist(),  # Convert to float16 and list for JSON
            'df_raw': self.df_raw.to_dict('records')  # Convert DataFrame to dict
        }

        # Use gzip compression with maximum compression level
        with gzip.open(cache_file, 'wt', compresslevel=9) as f:
            json.dump(cache_data, f, separators=(',', ':'))  # Compact JSON

        logger.info(f"Compressed RAG data saved to {cache_file}")

        # Log file size
        size_mb = os.path.getsize(cache_file) / (1024 * 1024)
        logger.info(f"Cache file size: {size_mb:.2f} MB")

    def load_compressed_rag_cache(self, cache_file='rag_cache_compressed.gz'):
        """Load RAG data from compressed file"""
        try:
            with gzip.open(cache_file, 'rt') as f:
                cache_data = json.load(f)

            self.rag_texts = cache_data['texts']
            self.rag_embeddings = np.array(cache_data['embeddings'], dtype=np.float32)  # Convert back to numpy
            self.df_raw = pd.DataFrame(cache_data['df_raw'])

            # Load embedder
            self.embedder = SentenceTransformer('all-MiniLM-L6-v2')

            logger.info("Compressed RAG data loaded successfully")
            return True
        except Exception as e:
            logger.warning(f"Failed to load compressed cache: {e}")
            return False

    def prepare_rag_minimal(self, df_raw, max_samples=500):
        """Prepare RAG with minimal data for GitHub storage"""
        logger.info(f"Preparing minimal RAG data (max {max_samples} samples)...")

        # Sample diverse data to maintain quality
        df_sampled = self._smart_sample(df_raw, max_samples)

        df_sampled = df_sampled.fillna('unknown')
        self.rag_texts = [self.row_to_text_optimized(row) for _, row in df_sampled.iterrows()]
        self.df_raw = df_sampled

        logger.info("Loading sentence transformer...")
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')

        logger.info("Generating embeddings...")
        self.rag_embeddings = self.embedder.encode(self.rag_texts, show_progress_bar=True)

        # Save with compression
        self.save_compressed_rag_cache()

    def _smart_sample(self, df, max_samples):
        """Smart sampling to maintain diversity"""
        if len(df) <= max_samples:
            return df

        # Stratified sampling based on loan status and key features
        try:
            # Sample equal numbers from approved/rejected
            approved = df[df['Loan_Status'] == 'Y']
            rejected = df[df['Loan_Status'] == 'N']

            n_approved = min(len(approved), max_samples // 2)
            n_rejected = min(len(rejected), max_samples - n_approved)

            sampled_approved = approved.sample(n=n_approved, random_state=42)
            sampled_rejected = rejected.sample(n=n_rejected, random_state=42)

            return pd.concat([sampled_approved, sampled_rejected]).reset_index(drop=True)

        except Exception as e:
            logger.warning(f"Smart sampling failed: {e}. Using random sampling.")
            return df.sample(n=max_samples, random_state=42).reset_index(drop=True)

    def prepare_rag_optimized(self, df_raw, cache_file='rag_cache_compressed.gz', force_rebuild=False):
        """Optimized RAG preparation with compression"""

        if not force_rebuild and os.path.exists(cache_file):
            logger.info("Loading compressed RAG data from cache...")
            if self.load_compressed_rag_cache(cache_file):
                return

        # If cache doesn't exist or loading failed, create minimal version
        self.prepare_rag_minimal(df_raw)

    @lru_cache(maxsize=128)
    def get_stats_summary(self, query_lower):
        """Cached statistics summary generation"""
        summary = ""
        conclusion = ""

        keyword_checks = {
            'self-employed': ('Self_Employed', 'Yes'),
            'married': ('Married', 'Yes'),
            'graduate': ('Education', 'Graduate'),
            'not graduate': ('Education', 'Not Graduate'),
            'male': ('Gender', 'Male'),
            'female': ('Gender', 'Female'),
            'urban': ('Property_Area', 'Urban'),
            'rural': ('Property_Area', 'Rural'),
            'semiurban': ('Property_Area', 'Semiurban')
        }

        for keyword, (column, value) in keyword_checks.items():
            if keyword in query_lower and column in self.train_stats:
                summary += "\n\n📊 **Statistical Summary**:\n"
                stats = self.train_stats[column]
                sorted_stats = sorted(stats.items(), key=lambda x: x[1], reverse=True)

                for category, rate in sorted_stats:
                    summary += f"- {category}: {rate:.1f}% approval\n"

                if value in stats:
                    target_rate = stats[value]
                    avg_rate = np.mean(list(stats.values()))

                    if target_rate > avg_rate:
                        conclusion = f"\n✅ **Conclusion**: Yes, {keyword} applicants have higher approval rates ({target_rate:.1f}%)."
                    else:
                        conclusion = f"\n❌ **Conclusion**: {keyword} applicants have lower approval rates ({target_rate:.1f}%)."
                break

        return summary, conclusion

    def answer_query_optimized(self, query, top_k=3):
        """Optimized query answering"""
        if self.embedder is None or self.rag_embeddings is None:
            return "RAG system not initialized. Please run prepare_rag_optimized first."

        q_embed = self.embedder.encode([query])

        scores = cosine_similarity(q_embed, self.rag_embeddings)[0]
        top_indices = np.argpartition(scores, -top_k)[-top_k:]
        top_indices = top_indices[np.argsort(scores[top_indices])[::-1]]

        context = "\n".join([self.rag_texts[i] for i in top_indices])

        query_lower = query.lower()
        summary, conclusion = self.get_stats_summary(query_lower)

        return f"Based on similar applicants:\n{context}{summary}{conclusion}"

    def save_model(self, filepath='loan_model.pkl'):
        """Save the entire pipeline"""
        pipeline_data = {
            'model': self.model,
            'label_encoders': self.label_encoders,
            'train_stats': self.train_stats
        }

        with open(filepath, 'wb') as f:
            pickle.dump(pipeline_data, f)
        logger.info(f"Model saved to {filepath}")

    def load_model(self, filepath='loan_model.pkl'):
        """Load the entire pipeline"""
        with open(filepath, 'rb') as f:
            pipeline_data = pickle.load(f)

        self.model = pipeline_data['model']
        self.label_encoders = pipeline_data['label_encoders']
        self.train_stats = pipeline_data['train_stats']
        logger.info(f"Model loaded from {filepath}")

def main():
    """Main execution function"""
    pipeline = OptimizedLoanPipeline()

    X_train, y_train, df_raw = pipeline.load_and_preprocess_train()
    X_test, loan_ids = pipeline.load_and_preprocess_test()

    preds = pipeline.train_and_predict(X_train, y_train, X_test)

    submission = pd.DataFrame({
        'Loan_ID': loan_ids,
        'Loan_Status': preds
    })
    submission.to_csv("submission.csv", index=False)
    logger.info("Predictions saved to submission.csv")

    # Use compressed RAG preparation
    pipeline.prepare_rag_optimized(df_raw)

    pipeline.save_model()

    sample_queries = [
        "What kind of people are more likely to get approved?",
        "Are married applicants more likely to get loans?",
        "Do graduates have better approval rates?"
    ]

    print("\n🧠 Sample RAG Responses:")
    for query in sample_queries:
        print(f"\n📝 Query: {query}")
        print("📋 Answer:", pipeline.answer_query_optimized(query))

if __name__ == "__main__":
    main()

